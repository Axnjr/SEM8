# Reinforcement Learning (RL)

# Introduction to Reinforcement Learning (04 Hours)

**1. Key features and elements of Reinforcement Learning (RL)**  
Reinforcement Learning is a type of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative reward. The agent observes the state of the environment, takes actions, and receives feedback in the form of rewards.

- **Agent:** The learner or decision maker.
- **Environment:** The external system the agent interacts with.
- **State (s):** A representation of the current situation of the environment.
- **Action (a):** Choices the agent can make.
- **Reward (r):** Feedback from the environment for each action taken.
- **Policy (π):** The strategy that the agent employs to determine actions based on states.
- **Value Function (V):** Estimates the expected cumulative reward from a state.
- **Model (optional):** Predicts environment dynamics (used in model-based RL).

**2. Types of RL**  
- **Model-Free RL:** In Model-Free Reinforcement Learning, the agent learns directly from its interactions with the environment without constructing an explicit model of the environment's dynamics. This approach is particularly useful when the environment is complex or unknown, making it difficult to model accurately. Algorithms like Q-learning and SARSA fall under this category. These algorithms focus on learning the value of actions or state-action pairs based on the rewards received, allowing the agent to improve its policy through trial and error.

- **Model-Based RL:** Model-Based Reinforcement Learning involves the agent building an internal model of the environment's dynamics. This model is used to simulate the environment and plan future actions. The agent can predict the outcomes of actions and evaluate different strategies before executing them in the real environment. This approach can be more sample-efficient than model-free methods because it allows the agent to leverage the model for planning and decision-making. However, constructing an accurate model can be challenging, especially in complex environments.

- **On-Policy RL:** On-Policy Reinforcement Learning refers to methods where the agent learns the value of the policy it is currently following. This means that the agent updates its knowledge based on the actions it takes according to its current policy. SARSA (State-Action-Reward-State-Action) is a common on-policy algorithm. On-policy methods are beneficial when the goal is to improve the current policy directly, as they ensure that the learning process is consistent with the policy being evaluated.

- **Off-Policy RL:** Off-Policy Reinforcement Learning involves learning the value of an optimal policy independently of the actions taken by the agent. This allows the agent to learn from actions that are outside of its current policy, often using data generated by a different policy. Q-learning is a well-known off-policy algorithm. Off-policy methods are advantageous because they can utilize data from various sources, including past experiences or other agents, to learn an optimal policy, potentially speeding up the learning process and improving performance.

**3. Rewards**  
Rewards are scalar feedback signals received after taking actions. The goal of the agent is to maximize the total cumulative reward (return) over time. Rewards can be immediate or delayed, and their design is crucial for effective learning.

**4. Reinforcement Learning Algorithms**

- **Q-Learning:**
  is a model-free reinforcement learning algorithm used to train agents (computer programs) to make optimal decisions by interacting with an environment. It helps the agent explore different actions and learn which ones lead to better outcomes. The agent uses trial and error to determine which actions result in rewards (good outcomes) or penalties (bad outcomes).

  Over time, it improves its decision-making by updating a Q-table, which stores Q-values representing the expected rewards for taking particular actions in given states. The Q-table is essentially a memory structure where the agent stores information about which actions yield the best rewards in each state.
    - Structure of a Q-table:
      - Rows represent the states.
      - Columns represent the possible actions.
      - Each entry in the table corresponds to the Q-value for a state-action pair.

  Key Components of Q-learning
    1. Q-Values or Action-Values
    represent the expected rewards for taking an action in a specific state. These values are updated over time using the Temporal Difference (TD) update rule.

    2. Rewards and Episodes
    The agent moves through different states by taking actions and receiving rewards. The process continues until the agent reaches a terminal state, which ends the episode.

    3. Temporal Difference or TD-Update
    The agent updates Q-values using the formula:
    ![alt text](td.png)

    4. ϵ-greedy Policy (Exploration vs. Exploitation)
    The ϵ-greedy policy helps the agent decide which action to take based on the current Q-value estimates:
      - Exploitation: The agent picks the action with the highest Q-value with probability `1–ϵ`. 
      This means the agent uses its current knowledge to maximize rewards.
      - Exploration: With probability ϵ, the agent picks a random action, exploring new possibilities to learn if there are better ways to get rewards. This allows the agent to discover new strategies and improve its decision-making over time.

  - How does Q-Learning Works?
  Q-learning models follow an iterative process, where different components work together to train the agent, Steps of Q-learning:
    - Initialization: The agent starts with an initial Q-table, where Q-values are typically initialized to zero.
    - Exploration: The agent chooses an action based on the ϵ-greedy policy (either exploring or exploiting).
    - Action and Update: The agent takes the action, observes the next state, and receives a reward. The Q-value for the state-action pair is updated using the TD update rule.
    - Iteration: The process repeats for multiple episodes until the agent learns the optimal policy.

- **SARSA (State-Action-Reward-State-Action):**
  - SARSA is a model-free, on-policy reinforcement learning algorithm. Unlike Q-Learning, SARSA updates its Q-values based on the action actually taken by the current policy, making it an on-policy method. `The core idea of SARSA is to update the Q-value for each state-action pair based on the actual experience (i.e., what the agent does while following its policy). The Q-value is updated using the following Bellman Equation for SARSA: (above ☝️)`
  This means that the learning process is consistent with the policy being evaluated, as the agent learns from the actions it takes according to its current policy.
  - The update rule for SARSA is given by: `(above ☝️)`
  - Here, the parameters are similar to those in Q-Learning: \( \alpha \) is the learning rate, \( \gamma \) is the discount factor, and \( r \) is the reward. However, the key difference is that the next action \( a' \) is chosen according to the current policy, rather than being the action that maximizes the Q-value. This makes SARSA sensitive to the exploration strategy used, as it directly affects the actions taken and thus the learning process.

**Summary:**
Reinforcement Learning provides a framework for agents to learn optimal behaviors through trial and error, guided by rewards. Key algorithms like Q-learning and SARSA enable agents to learn value functions and policies for decision making in uncertain environments. 